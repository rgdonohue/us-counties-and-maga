{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Processing & Harmonization\n",
    "\n",
    "This notebook processes raw data into analysis-ready format with consistent FIPS codes, time windows, and calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from scipy import stats\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent\n",
    "data_raw = project_root / 'data' / 'raw'\n",
    "data_processed = project_root / 'data' / 'processed'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Electoral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_election_data(year):\n",
    "    \"\"\"Process county presidential election returns\"\"\"\n",
    "    \n",
    "    # Load raw data\n",
    "    file_path = data_raw / 'elections' / f'county_presidential_{year}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Create FIPS code (state + county)\n",
    "    df['fips'] = df['state_fips'].astype(str).str.zfill(2) + df['county_fips'].astype(str).str.zfill(3)\n",
    "    \n",
    "    # Filter for major candidates\n",
    "    trump_votes = df[df['candidate'].str.contains('TRUMP', case=False, na=False)].groupby('fips')['candidatevotes'].sum()\n",
    "    \n",
    "    if year == 2016:\n",
    "        dem_votes = df[df['candidate'].str.contains('CLINTON', case=False, na=False)].groupby('fips')['candidatevotes'].sum()\n",
    "    else:\n",
    "        dem_votes = df[df['candidate'].str.contains('BIDEN', case=False, na=False)].groupby('fips')['candidatevotes'].sum()\n",
    "    \n",
    "    total_votes = df.groupby('fips')['candidatevotes'].sum()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = pd.DataFrame({\n",
    "        f'trump_votes_{year}': trump_votes,\n",
    "        f'dem_votes_{year}': dem_votes,\n",
    "        f'total_votes_{year}': total_votes,\n",
    "        f'trump_share_{year}': trump_votes / (trump_votes + dem_votes) * 100,\n",
    "        f'trump_margin_{year}': ((trump_votes - dem_votes) / total_votes * 100)\n",
    "    })\n",
    "    \n",
    "    results.index.name = 'fips'\n",
    "    results = results.reset_index()\n",
    "    \n",
    "    logger.info(f\"Processed {year} election data: {len(results)} counties\")\n",
    "    return results\n",
    "\n",
    "# Process 2016 and 2020 elections\n",
    "# election_2016 = process_election_data(2016)\n",
    "# election_2020 = process_election_data(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process CDC WONDER Mortality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cdc_wonder(file_path, metric_name):\n",
    "    \"\"\"Process CDC WONDER mortality data files\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "    # Clean county names and extract FIPS\n",
    "    df['fips'] = df['County Code'].astype(str).str.zfill(5)\n",
    "    \n",
    "    # Handle suppressed data (marked as 'Suppressed' or < 10 deaths)\n",
    "    df['Deaths'] = pd.to_numeric(df['Deaths'], errors='coerce')\n",
    "    df['Age Adjusted Rate'] = pd.to_numeric(df['Age Adjusted Rate'], errors='coerce')\n",
    "    \n",
    "    # Calculate period averages\n",
    "    period_avg = df.groupby('fips').agg({\n",
    "        'Deaths': 'sum',\n",
    "        'Population': 'mean',\n",
    "        'Age Adjusted Rate': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    period_avg.columns = [f'{metric_name}_deaths', f'{metric_name}_pop', f'{metric_name}_rate']\n",
    "    \n",
    "    logger.info(f\"Processed {metric_name}: {len(period_avg)} counties\")\n",
    "    return period_avg.reset_index()\n",
    "\n",
    "# Process each mortality type\n",
    "# overdose_1316 = process_cdc_wonder(data_raw / 'cdc_wonder/overdose_2013_2016.txt', 'od_1316')\n",
    "# overdose_1720 = process_cdc_wonder(data_raw / 'cdc_wonder/overdose_2017_2020.txt', 'od_1720')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process CDC PLACES Health Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cdc_places():\n",
    "    \"\"\"Process CDC PLACES county-level health data\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_raw / 'cdc_places/places_county_2023.csv')\n",
    "    \n",
    "    # Filter for county-level data\n",
    "    county_df = df[df['DataValueTypeID'] == 'AgeAdjPrv'].copy()\n",
    "    \n",
    "    # Select pain-related indicators\n",
    "    pain_indicators = {\n",
    "        'ARTHRITIS': 'arthritis_pct',\n",
    "        'PHLTH': 'freq_phys_distress_pct',\n",
    "        'DISABILITY': 'disability_pct',\n",
    "        'DEPRESSION': 'depression_pct',\n",
    "        'BPHIGH': 'high_bp_pct',\n",
    "        'DIABETES': 'diabetes_pct'\n",
    "    }\n",
    "    \n",
    "    # Pivot to wide format\n",
    "    places_wide = county_df[county_df['MeasureId'].isin(pain_indicators.keys())].pivot(\n",
    "        index='LocationID',\n",
    "        columns='MeasureId',\n",
    "        values='Data_Value'\n",
    "    )\n",
    "    \n",
    "    # Rename columns\n",
    "    places_wide.rename(columns=pain_indicators, inplace=True)\n",
    "    \n",
    "    # Extract FIPS from LocationID\n",
    "    places_wide.index = places_wide.index.astype(str).str.zfill(5)\n",
    "    places_wide.index.name = 'fips'\n",
    "    \n",
    "    logger.info(f\"Processed CDC PLACES: {len(places_wide)} counties, {places_wide.shape[1]} indicators\")\n",
    "    return places_wide.reset_index()\n",
    "\n",
    "# places_data = process_cdc_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process USDA Rural-Urban Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rucc_codes():\n",
    "    \"\"\"Process USDA Rural-Urban Continuum Codes\"\"\"\n",
    "    \n",
    "    df = pd.read_excel(data_raw / 'usda/rucc_2023.xlsx')\n",
    "    \n",
    "    # Create FIPS code\n",
    "    df['fips'] = df['FIPS'].astype(str).str.zfill(5)\n",
    "    \n",
    "    # Create binary rural indicator (RUCC >= 4)\n",
    "    df['rural'] = (df['RUCC_2023'] >= 4).astype(int)\n",
    "    \n",
    "    # Create categories\n",
    "    df['rucc_category'] = pd.cut(\n",
    "        df['RUCC_2023'],\n",
    "        bins=[0, 3, 6, 9],\n",
    "        labels=['Metro', 'Micropolitan', 'Rural']\n",
    "    )\n",
    "    \n",
    "    # Select columns\n",
    "    rucc_processed = df[['fips', 'RUCC_2023', 'rural', 'rucc_category']].copy()\n",
    "    rucc_processed.columns = ['fips', 'rucc', 'rural', 'rucc_category']\n",
    "    \n",
    "    logger.info(f\"Processed RUCC: {len(rucc_processed)} counties\")\n",
    "    return rucc_processed\n",
    "\n",
    "# rucc_data = process_rucc_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data():\n",
    "    \"\"\"Merge all processed data sources into a single analysis dataset\"\"\"\n",
    "    \n",
    "    # Load county boundaries as base\n",
    "    counties = gpd.read_file(data_raw / 'shapefiles/tl_2023_us_county.shp')\n",
    "    counties['fips'] = counties['GEOID']\n",
    "    \n",
    "    # Filter to continental US (exclude territories, Alaska, Hawaii)\n",
    "    counties = counties[~counties['STATEFP'].isin(['02', '15', '60', '66', '69', '72', '78'])]\n",
    "    \n",
    "    # Start with geometry\n",
    "    merged = counties[['fips', 'NAME', 'STATEFP', 'geometry']].copy()\n",
    "    \n",
    "    # List of dataframes to merge\n",
    "    # This would include all the processed dataframes from above\n",
    "    # dfs_to_merge = [\n",
    "    #     election_2016, election_2020,\n",
    "    #     overdose_1316, overdose_1720,\n",
    "    #     suicide_1316, suicide_1720,\n",
    "    #     places_data,\n",
    "    #     rucc_data,\n",
    "    #     census_data\n",
    "    # ]\n",
    "    \n",
    "    # for df in dfs_to_merge:\n",
    "    #     merged = merged.merge(df, on='fips', how='left')\n",
    "    \n",
    "    # Calculate derived variables\n",
    "    # merged['trump_shift_16_20'] = merged['trump_share_2020'] - merged['trump_share_2016']\n",
    "    # merged['od_rate_change'] = merged['od_1720_rate'] - merged['od_1316_rate']\n",
    "    \n",
    "    logger.info(f\"Final dataset: {len(merged)} counties, {merged.shape[1]} variables\")\n",
    "    return merged\n",
    "\n",
    "# final_data = merge_all_data()\n",
    "# final_data.to_file(data_processed / 'counties_analysis.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df):\n",
    "    \"\"\"Generate data quality report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'n_counties': len(df),\n",
    "        'n_variables': df.shape[1],\n",
    "        'missing_by_column': df.isnull().sum().to_dict(),\n",
    "        'pct_missing_by_column': (df.isnull().sum() / len(df) * 100).round(2).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Identify counties with high missingness\n",
    "    row_missingness = df.isnull().sum(axis=1)\n",
    "    high_missing = df[row_missingness > df.shape[1] * 0.3][['fips', 'NAME']]\n",
    "    report['high_missing_counties'] = high_missing.to_dict('records')\n",
    "    \n",
    "    # Check for outliers in key variables\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outliers = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].notna().sum() > 0:\n",
    "            z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "            outliers[col] = (z_scores > 3).sum()\n",
    "    \n",
    "    report['outliers_by_column'] = outliers\n",
    "    \n",
    "    return report\n",
    "\n",
    "# quality_report = data_quality_report(final_data)\n",
    "# pd.DataFrame([quality_report]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}