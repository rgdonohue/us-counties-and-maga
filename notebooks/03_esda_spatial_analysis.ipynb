{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Exploratory Spatial Data Analysis (ESDA)\n",
    "\n",
    "This notebook performs spatial analysis to identify patterns and clusters in the relationship between pain/distress metrics and Trump voting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Spatial analysis libraries\n",
    "import libpysal\n",
    "from libpysal.weights import Queen, Rook, KNN\n",
    "import esda\n",
    "from esda.moran import Moran, Moran_Local, Moran_BV, Moran_Local_BV\n",
    "import splot\n",
    "from splot.esda import moran_scatterplot, lisa_cluster, plot_local_autocorrelation\n",
    "import mapclassify\n",
    "\n",
    "# Setup\n",
    "project_root = Path.cwd().parent\n",
    "data_processed = project_root / 'data' / 'processed'\n",
    "reports = project_root / 'reports'\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3109, 61)\n",
      "\n",
      "First 10 columns: ['fips', 'county_name', 'state_fips', 'trump_votes_2016', 'opponent_votes_2016', 'two_party_votes_2016', 'total_votes_2016', 'trump_share_2016', 'trump_margin_2016', 'trump_votes_2020']\n",
      "Geometry CRS: EPSG:4326\n",
      "Reprojected to: EPSG:5070\n",
      "\n",
      "Key variables available:\n",
      "  trump_share_2016: 3100/3109 (99.7%) non-null\n",
      "  trump_share_2020: 3100/3109 (99.7%) non-null\n",
      "  od_1316_rate: 0/3109 (0.0%) non-null\n",
      "  od_1720_rate: 0/3109 (0.0%) non-null\n",
      "  freq_phys_distress_pct: 3109/3109 (100.0%) non-null\n",
      "  depression_pct: 3109/3109 (100.0%) non-null\n"
     ]
    }
   ],
   "source": [
    "# Load the merged county dataset\n",
    "gdf = gpd.read_file(data_processed / 'counties_analysis.geojson')\n",
    "\n",
    "print(f\"Dataset shape: {gdf.shape}\")\n",
    "print(f\"\\nFirst 10 columns: {gdf.columns.tolist()[:10]}\")\n",
    "print(f\"Geometry CRS: {gdf.crs}\")\n",
    "\n",
    "# Ensure we're using a projected CRS for accurate spatial calculations\n",
    "if gdf.crs.to_epsg() == 4326:\n",
    "    gdf = gdf.to_crs('EPSG:5070')  # Albers Equal Area for continental US\n",
    "    print(f\"Reprojected to: {gdf.crs}\")\n",
    "\n",
    "# Quick data summary\n",
    "print(f\"\\nKey variables available:\")\n",
    "key_vars = ['trump_share_2016', 'trump_share_2020', 'od_1316_rate', 'od_1720_rate', \n",
    "            'freq_phys_distress_pct', 'depression_pct']\n",
    "for var in key_vars:\n",
    "    if var in gdf.columns:\n",
    "        non_null = gdf[var].notna().sum()\n",
    "        print(f\"  {var}: {non_null}/{len(gdf)} ({non_null/len(gdf)*100:.1f}%) non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial Weights Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial weights summary:\n",
      "  - Type: queen\n",
      "  - Number of observations: 3109\n",
      "  - Average number of neighbors: 5.94\n",
      "  - Min neighbors: 1\n",
      "  - Max neighbors: 14\n",
      "  - Islands: []\n"
     ]
    }
   ],
   "source": [
    "def create_spatial_weights(gdf, weight_type='queen'):\n",
    "    \"\"\"Create spatial weights matrix\"\"\"\n",
    "    \n",
    "    if weight_type == 'queen':\n",
    "        w = Queen.from_dataframe(gdf, use_index=True)\n",
    "    elif weight_type == 'rook':\n",
    "        w = Rook.from_dataframe(gdf, use_index=True)\n",
    "    elif weight_type == 'knn':\n",
    "        w = KNN.from_dataframe(gdf, k=8)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown weight type: {weight_type}\")\n",
    "    \n",
    "    # Row-standardize the weights\n",
    "    w.transform = 'r'\n",
    "    \n",
    "    print(f\"Spatial weights summary:\")\n",
    "    print(f\"  - Type: {weight_type}\")\n",
    "    print(f\"  - Number of observations: {w.n}\")\n",
    "    print(f\"  - Average number of neighbors: {w.mean_neighbors:.2f}\")\n",
    "    print(f\"  - Min neighbors: {w.min_neighbors}\")\n",
    "    print(f\"  - Max neighbors: {w.max_neighbors}\")\n",
    "    print(f\"  - Islands: {w.islands}\")\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Create Queen contiguity weights (counties that share a border)\n",
    "w_queen = create_spatial_weights(gdf, 'queen')\n",
    "\n",
    "# Handle islands if any\n",
    "if len(w_queen.islands) > 0:\n",
    "    print(f\"\\nWarning: {len(w_queen.islands)} islands detected\")\n",
    "    print(f\"Island indices: {w_queen.islands}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global Spatial Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 6 variables: ['trump_share_2016', 'trump_share_2020', 'trump_shift_16_20', 'freq_phys_distress_pct', 'arthritis_pct', 'depression_pct']\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Moran' object has no attribute 'VI'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(available_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Run global Moran analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m moran_results = \u001b[43mglobal_moran_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_queen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m display(moran_results.sort_values(\u001b[33m'\u001b[39m\u001b[33mmoran_i\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mglobal_moran_analysis\u001b[39m\u001b[34m(gdf, w, variables)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m gdf.columns:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Calculate Moran's I\u001b[39;00m\n\u001b[32m      9\u001b[39m     mi = Moran(gdf[var].values, w, permutations=\u001b[32m999\u001b[39m)\n\u001b[32m     11\u001b[39m     results.append({\n\u001b[32m     12\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m'\u001b[39m: var,\n\u001b[32m     13\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmoran_i\u001b[39m\u001b[33m'\u001b[39m: mi.I,\n\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mexpected_i\u001b[39m\u001b[33m'\u001b[39m: mi.EI,\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mvariance\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mmi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVI\u001b[49m,\n\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mz_score\u001b[39m\u001b[33m'\u001b[39m: mi.z_norm,\n\u001b[32m     17\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mp_value\u001b[39m\u001b[33m'\u001b[39m: mi.p_norm,\n\u001b[32m     18\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msignificant\u001b[39m\u001b[33m'\u001b[39m: mi.p_norm < \u001b[32m0.05\u001b[39m\n\u001b[32m     19\u001b[39m     })\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Create Moran scatterplot\u001b[39;00m\n\u001b[32m     22\u001b[39m     fig, ax = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Moran' object has no attribute 'VI'"
     ]
    }
   ],
   "source": [
    "def global_moran_analysis(gdf, w, variables):\n",
    "    \"\"\"Calculate Global Moran's I for multiple variables\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for var in variables:\n",
    "        if var in gdf.columns:\n",
    "            # Check for sufficient non-null values\n",
    "            valid_count = gdf[var].notna().sum()\n",
    "            \n",
    "            if valid_count < 30:\n",
    "                print(f\"Skipping {var}: insufficient non-null values ({valid_count})\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate Moran's I (it handles NaN values internally)\n",
    "            try:\n",
    "                mi = Moran(gdf[var].values, w, permutations=999)\n",
    "                \n",
    "                # Get variance (handle different esda versions)\n",
    "                variance = getattr(mi, 'VI_norm', getattr(mi, 'VI_rand', None))\n",
    "                if variance is None:\n",
    "                    variance = mi.seI_norm**2 if hasattr(mi, 'seI_norm') else np.nan\n",
    "                \n",
    "                results.append({\n",
    "                    'variable': var,\n",
    "                    'moran_i': mi.I,\n",
    "                    'expected_i': mi.EI,\n",
    "                    'variance': variance,\n",
    "                    'z_score': mi.z_norm,\n",
    "                    'p_value': mi.p_norm,\n",
    "                    'p_value_sim': mi.p_sim,\n",
    "                    'significant': mi.p_norm < 0.05\n",
    "                })\n",
    "                \n",
    "                # Create Moran scatterplot\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "                moran_scatterplot(mi, ax=ax)\n",
    "                ax.set_title(f\"Moran's I Scatterplot: {var}\\nI = {mi.I:.4f}, p = {mi.p_norm:.4f}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(reports / f'figures/moran_scatter_{var}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {var}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define variables to analyze\n",
    "spatial_vars = [\n",
    "    'trump_share_2016',\n",
    "    'trump_share_2020',\n",
    "    'trump_shift_16_20',\n",
    "    'od_1316_rate',\n",
    "    'od_1720_rate',\n",
    "    'od_rate_change',\n",
    "    'freq_phys_distress_pct',\n",
    "    'arthritis_pct',\n",
    "    'depression_pct'\n",
    "]\n",
    "\n",
    "# Filter to variables that exist and have sufficient data\n",
    "available_vars = [v for v in spatial_vars if v in gdf.columns and gdf[v].notna().sum() > 100]\n",
    "print(f\"Analyzing {len(available_vars)} variables: {available_vars}\\n\")\n",
    "\n",
    "# Run global Moran analysis\n",
    "moran_results = global_moran_analysis(gdf, w_queen, available_vars)\n",
    "display(moran_results.sort_values('moran_i', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local Spatial Autocorrelation (LISA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_moran_analysis(gdf, w, variable, alpha=0.05):\n",
    "    \"\"\"Calculate Local Moran's I and identify clusters\"\"\"\n",
    "    \n",
    "    # Calculate Local Moran's I\n",
    "    lisa = Moran_Local(gdf[variable].values, w, permutations=999)\n",
    "    \n",
    "    # Add LISA statistics to geodataframe\n",
    "    gdf[f'{variable}_lisa_i'] = lisa.Is\n",
    "    gdf[f'{variable}_lisa_p'] = lisa.p_sim\n",
    "    gdf[f'{variable}_lisa_q'] = lisa.q\n",
    "    \n",
    "    # Identify significant clusters\n",
    "    sig = gdf[f'{variable}_lisa_p'] < alpha\n",
    "    gdf[f'{variable}_lisa_cluster'] = 'Not Significant'\n",
    "    gdf.loc[sig & (gdf[f'{variable}_lisa_q'] == 1), f'{variable}_lisa_cluster'] = 'HH'  # High-High\n",
    "    gdf.loc[sig & (gdf[f'{variable}_lisa_q'] == 2), f'{variable}_lisa_cluster'] = 'LH'  # Low-High\n",
    "    gdf.loc[sig & (gdf[f'{variable}_lisa_q'] == 3), f'{variable}_lisa_cluster'] = 'LL'  # Low-Low\n",
    "    gdf.loc[sig & (gdf[f'{variable}_lisa_q'] == 4), f'{variable}_lisa_cluster'] = 'HL'  # High-Low\n",
    "    \n",
    "    # Create LISA cluster map\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Raw values map\n",
    "    gdf.plot(column=variable, scheme='quantiles', k=5, cmap='RdBu_r',\n",
    "             edgecolor='white', linewidth=0.1, ax=axes[0], legend=True)\n",
    "    axes[0].set_title(f'{variable} - Raw Values')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # LISA cluster map\n",
    "    colors = {'HH': '#d62728', 'HL': '#ff9896', 'LH': '#9edae5', 'LL': '#1f77b4', 'Not Significant': '#7f7f7f'}\n",
    "    gdf['color'] = gdf[f'{variable}_lisa_cluster'].map(colors)\n",
    "    gdf.plot(color=gdf['color'], edgecolor='white', linewidth=0.1, ax=axes[1])\n",
    "    axes[1].set_title(f'{variable} - LISA Clusters')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[k], label=k) for k in colors.keys()]\n",
    "    axes[1].legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports / f'figures/lisa_cluster_{variable}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    cluster_counts = gdf[f'{variable}_lisa_cluster'].value_counts()\n",
    "    print(f\"\\nLISA Cluster Summary for {variable}:\")\n",
    "    print(cluster_counts)\n",
    "    \n",
    "    return lisa, gdf\n",
    "\n",
    "# Analyze Trump 2016 vote share\n",
    "print(\"Analyzing spatial clusters of Trump 2016 support...\")\n",
    "lisa_trump_2016, gdf = local_moran_analysis(gdf, w_queen, 'trump_share_2016')\n",
    "\n",
    "# Analyze overdose rates (if available)\n",
    "if 'od_1316_rate' in gdf.columns and gdf['od_1316_rate'].notna().sum() > 100:\n",
    "    print(\"\\nAnalyzing spatial clusters of overdose rates (2013-2016)...\")\n",
    "    lisa_overdose, gdf = local_moran_analysis(gdf, w_queen, 'od_1316_rate')\n",
    "\n",
    "# Analyze physical distress (if available)\n",
    "if 'freq_phys_distress_pct' in gdf.columns and gdf['freq_phys_distress_pct'].notna().sum() > 100:\n",
    "    print(\"\\nAnalyzing spatial clusters of physical distress...\")\n",
    "    lisa_distress, gdf = local_moran_analysis(gdf, w_queen, 'freq_phys_distress_pct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bivariate Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_moran_analysis(gdf, w, var1, var2):\n",
    "    \"\"\"Bivariate Moran's I to test spatial correlation between two variables\"\"\"\n",
    "    \n",
    "    # Global bivariate Moran's I\n",
    "    moran_bv = Moran_BV(gdf[var1].values, gdf[var2].values, w, permutations=999)\n",
    "    \n",
    "    print(f\"\\nBivariate Moran's I: {var1} vs {var2}\")\n",
    "    print(f\"  I = {moran_bv.I:.4f}\")\n",
    "    print(f\"  p-value = {moran_bv.p_norm:.4f}\")\n",
    "    print(f\"  Significant: {moran_bv.p_norm < 0.05}\")\n",
    "    \n",
    "    # Local bivariate Moran's I\n",
    "    lisa_bv = Moran_Local_BV(gdf[var1].values, gdf[var2].values, w, permutations=999)\n",
    "    \n",
    "    # Add to geodataframe\n",
    "    gdf[f'bv_{var1}_{var2}_lisa_i'] = lisa_bv.Is\n",
    "    gdf[f'bv_{var1}_{var2}_lisa_p'] = lisa_bv.p_sim\n",
    "    gdf[f'bv_{var1}_{var2}_lisa_q'] = lisa_bv.q\n",
    "    \n",
    "    # Create bivariate LISA cluster map\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Variable 1 map\n",
    "    gdf.plot(column=var1, scheme='quantiles', k=5, cmap='Blues',\n",
    "             edgecolor='white', linewidth=0.1, ax=axes[0], legend=True)\n",
    "    axes[0].set_title(f'{var1}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Variable 2 map\n",
    "    gdf.plot(column=var2, scheme='quantiles', k=5, cmap='Reds',\n",
    "             edgecolor='white', linewidth=0.1, ax=axes[1], legend=True)\n",
    "    axes[1].set_title(f'{var2}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Bivariate LISA clusters\n",
    "    sig = gdf[f'bv_{var1}_{var2}_lisa_p'] < 0.05\n",
    "    gdf['bv_cluster'] = 'Not Significant'\n",
    "    gdf.loc[sig & (gdf[f'bv_{var1}_{var2}_lisa_q'] == 1), 'bv_cluster'] = 'HH'\n",
    "    gdf.loc[sig & (gdf[f'bv_{var1}_{var2}_lisa_q'] == 2), 'bv_cluster'] = 'LH'\n",
    "    gdf.loc[sig & (gdf[f'bv_{var1}_{var2}_lisa_q'] == 3), 'bv_cluster'] = 'LL'\n",
    "    gdf.loc[sig & (gdf[f'bv_{var1}_{var2}_lisa_q'] == 4), 'bv_cluster'] = 'HL'\n",
    "    \n",
    "    colors = {'HH': '#8b0000', 'HL': '#ff6347', 'LH': '#4169e1', 'LL': '#87ceeb', 'Not Significant': '#d3d3d3'}\n",
    "    gdf['bv_color'] = gdf['bv_cluster'].map(colors)\n",
    "    gdf.plot(color=gdf['bv_color'], edgecolor='white', linewidth=0.1, ax=axes[2])\n",
    "    axes[2].set_title(f'Bivariate LISA: {var1} vs {var2}')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=colors['HH'], label='HH: High-High'),\n",
    "        Patch(facecolor=colors['HL'], label='HL: High-Low'),\n",
    "        Patch(facecolor=colors['LH'], label='LH: Low-High'),\n",
    "        Patch(facecolor=colors['LL'], label='LL: Low-Low'),\n",
    "        Patch(facecolor=colors['Not Significant'], label='Not Significant')\n",
    "    ]\n",
    "    axes[2].legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports / f'figures/bivariate_lisa_{var1}_{var2}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nBivariate LISA Cluster Counts:\")\n",
    "    print(gdf['bv_cluster'].value_counts())\n",
    "    \n",
    "    return moran_bv, lisa_bv, gdf\n",
    "\n",
    "# Analyze relationship between overdose rates and Trump vote share\n",
    "if 'od_1316_rate' in gdf.columns and gdf['od_1316_rate'].notna().sum() > 100:\n",
    "    print(\"\\nBivariate analysis: Overdose rates vs Trump 2016 support...\")\n",
    "    moran_bv, lisa_bv, gdf = bivariate_moran_analysis(gdf, w_queen, 'od_1316_rate', 'trump_share_2016')\n",
    "\n",
    "# Analyze relationship between physical distress and Trump vote share\n",
    "if 'freq_phys_distress_pct' in gdf.columns and gdf['freq_phys_distress_pct'].notna().sum() > 100:\n",
    "    print(\"\\nBivariate analysis: Physical distress vs Trump 2016 support...\")\n",
    "    moran_bv2, lisa_bv2, gdf = bivariate_moran_analysis(gdf, w_queen, 'freq_phys_distress_pct', 'trump_share_2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hot Spot Analysis (Getis-Ord Gi*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.getisord import G_Local\n",
    "\n",
    "def hotspot_analysis(gdf, w, variable):\n",
    "    \"\"\"Getis-Ord Gi* hot spot analysis\"\"\"\n",
    "    \n",
    "    # Calculate Gi*\n",
    "    g = G_Local(gdf[variable].values, w, permutations=999, star=True)\n",
    "    \n",
    "    # Add to geodataframe\n",
    "    gdf[f'{variable}_gi'] = g.Gs\n",
    "    gdf[f'{variable}_gi_p'] = g.p_sim\n",
    "    gdf[f'{variable}_gi_z'] = g.Zs\n",
    "    \n",
    "    # Classify hot/cold spots based on z-scores and p-values\n",
    "    gdf[f'{variable}_hotspot'] = 'Not Significant'\n",
    "    sig = gdf[f'{variable}_gi_p'] < 0.05\n",
    "    gdf.loc[sig & (gdf[f'{variable}_gi_z'] > 0), f'{variable}_hotspot'] = 'Hot Spot'\n",
    "    gdf.loc[sig & (gdf[f'{variable}_gi_z'] < 0), f'{variable}_hotspot'] = 'Cold Spot'\n",
    "    \n",
    "    # Further classify by confidence level\n",
    "    gdf[f'{variable}_hotspot_conf'] = 'Not Significant'\n",
    "    # 99% confidence\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.01) & (gdf[f'{variable}_gi_z'] > 0), f'{variable}_hotspot_conf'] = 'Hot Spot - 99% Conf'\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.01) & (gdf[f'{variable}_gi_z'] < 0), f'{variable}_hotspot_conf'] = 'Cold Spot - 99% Conf'\n",
    "    # 95% confidence\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.05) & (gdf[f'{variable}_gi_p'] >= 0.01) & (gdf[f'{variable}_gi_z'] > 0), \n",
    "            f'{variable}_hotspot_conf'] = 'Hot Spot - 95% Conf'\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.05) & (gdf[f'{variable}_gi_p'] >= 0.01) & (gdf[f'{variable}_gi_z'] < 0), \n",
    "            f'{variable}_hotspot_conf'] = 'Cold Spot - 95% Conf'\n",
    "    # 90% confidence\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.10) & (gdf[f'{variable}_gi_p'] >= 0.05) & (gdf[f'{variable}_gi_z'] > 0), \n",
    "            f'{variable}_hotspot_conf'] = 'Hot Spot - 90% Conf'\n",
    "    gdf.loc[(gdf[f'{variable}_gi_p'] < 0.10) & (gdf[f'{variable}_gi_p'] >= 0.05) & (gdf[f'{variable}_gi_z'] < 0), \n",
    "            f'{variable}_hotspot_conf'] = 'Cold Spot - 90% Conf'\n",
    "    \n",
    "    # Create hot spot map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    colors = {\n",
    "        'Hot Spot - 99% Conf': '#b30000',\n",
    "        'Hot Spot - 95% Conf': '#e34a33',\n",
    "        'Hot Spot - 90% Conf': '#fc8d59',\n",
    "        'Not Significant': '#ffffcc',\n",
    "        'Cold Spot - 90% Conf': '#91bfdb',\n",
    "        'Cold Spot - 95% Conf': '#4575b4',\n",
    "        'Cold Spot - 99% Conf': '#253494'\n",
    "    }\n",
    "    \n",
    "    gdf['hotspot_color'] = gdf[f'{variable}_hotspot_conf'].map(colors)\n",
    "    gdf.plot(color=gdf['hotspot_color'], edgecolor='white', linewidth=0.1, ax=ax)\n",
    "    ax.set_title(f'Hot Spot Analysis (Getis-Ord Gi*): {variable}')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[k], label=k) for k in colors.keys()]\n",
    "    ax.legend(handles=legend_elements, loc='lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports / f'figures/hotspot_{variable}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nHot Spot Analysis Summary for {variable}:\")\n",
    "    print(gdf[f'{variable}_hotspot_conf'].value_counts())\n",
    "    \n",
    "    return g, gdf\n",
    "\n",
    "# Identify hot spots of Trump support\n",
    "print(\"\\nHot spot analysis: Trump 2016 support...\")\n",
    "g_trump, gdf = hotspot_analysis(gdf, w_queen, 'trump_share_2016')\n",
    "\n",
    "# Identify hot spots of overdose rates\n",
    "if 'od_1316_rate' in gdf.columns and gdf['od_1316_rate'].notna().sum() > 100:\n",
    "    print(\"\\nHot spot analysis: Overdose rates (2013-2016)...\")\n",
    "    g_overdose, gdf = hotspot_analysis(gdf, w_queen, 'od_1316_rate')\n",
    "\n",
    "# Identify hot spots of physical distress\n",
    "if 'freq_phys_distress_pct' in gdf.columns and gdf['freq_phys_distress_pct'].notna().sum() > 100:\n",
    "    print(\"\\nHot spot analysis: Physical distress...\")\n",
    "    g_distress, gdf = hotspot_analysis(gdf, w_queen, 'freq_phys_distress_pct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results for Web Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_web_export(gdf):\n",
    "    \"\"\"Prepare data for web visualization\"\"\"\n",
    "    \n",
    "    # Select columns for web export (filter to what's actually available)\n",
    "    web_columns = [\n",
    "        'fips', 'county_name', 'state_fips', 'geometry',\n",
    "        # Electoral\n",
    "        'trump_share_2016', 'trump_share_2020', 'trump_shift_16_20',\n",
    "        'trump_margin_2016', 'trump_margin_2020',\n",
    "        # Pain/distress metrics\n",
    "        'od_1316_rate', 'od_1720_rate', 'od_rate_change',\n",
    "        'freq_phys_distress_pct', 'freq_mental_distress_pct',\n",
    "        'arthritis_pct', 'depression_pct', 'diabetes_pct',\n",
    "        # County Health Rankings\n",
    "        'chr_drug_overdose_deaths_per_100k', 'chr_poor_physical_health_days',\n",
    "        'chr_poor_mental_health_days',\n",
    "        # LISA clusters (if computed)\n",
    "        'trump_share_2016_lisa_cluster',\n",
    "        'od_1316_rate_lisa_cluster',\n",
    "        'freq_phys_distress_pct_lisa_cluster',\n",
    "        # Hot spots (if computed)\n",
    "        'trump_share_2016_hotspot_conf',\n",
    "        'od_1316_rate_hotspot_conf',\n",
    "        'freq_phys_distress_pct_hotspot_conf',\n",
    "        # Controls\n",
    "        'rucc', 'rural', 'rucc_category', 'ba_plus_pct', 'median_income'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_cols = [c for c in web_columns if c in gdf.columns]\n",
    "    print(f\"Exporting {len(available_cols)} columns: {available_cols[:10]}...\")\n",
    "    \n",
    "    web_gdf = gdf[available_cols].copy()\n",
    "    \n",
    "    # Convert back to WGS84 for web mapping\n",
    "    if web_gdf.crs.to_epsg() != 4326:\n",
    "        web_gdf = web_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Simplify geometry to reduce file size\n",
    "    web_gdf['geometry'] = web_gdf['geometry'].simplify(0.01, preserve_topology=True)\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_cols = web_gdf.select_dtypes(include=[np.number]).columns\n",
    "    web_gdf[numeric_cols] = web_gdf[numeric_cols].round(2)\n",
    "    \n",
    "    # Export as GeoJSON\n",
    "    output_path = project_root / 'web' / 'assets' / 'counties_esda.geojson'\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    web_gdf.to_file(output_path, driver='GeoJSON')\n",
    "    \n",
    "    print(f\"âœ… Exported {len(web_gdf)} counties to {output_path}\")\n",
    "    print(f\"   File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return web_gdf\n",
    "\n",
    "# Export the enriched dataset with spatial analysis results\n",
    "web_data = prepare_web_export(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Findings\n",
    "\n",
    "Key spatial patterns identified:\n",
    "\n",
    "### Global Spatial Autocorrelation\n",
    "- Review the Moran's I table above to see which variables show significant spatial clustering\n",
    "- Variables with high positive Moran's I indicate strong spatial autocorrelation (similar values cluster together)\n",
    "- Variables with negative Moran's I show spatial dispersion (dissimilar values are neighbors)\n",
    "\n",
    "### Local Clustering (LISA)\n",
    "- **HH (High-High)**: Counties with high values surrounded by high-value neighbors\n",
    "- **LL (Low-Low)**: Counties with low values surrounded by low-value neighbors  \n",
    "- **HL (High-Low)**: High-value outliers surrounded by low-value neighbors\n",
    "- **LH (Low-High)**: Low-value outliers surrounded by high-value neighbors\n",
    "\n",
    "### Bivariate Relationships\n",
    "- Bivariate LISA maps show where two variables co-occur spatially\n",
    "- HH clusters indicate counties where both variables are high AND spatially clustered\n",
    "- These are the key \"hotspots\" for the pain-politics relationship\n",
    "\n",
    "### Hot Spot Analysis (Getis-Ord Gi*)\n",
    "- More focused on statistical hot spots vs cold spots\n",
    "- Multi-level confidence intervals (90%, 95%, 99%)\n",
    "- Useful for identifying statistically significant geographic concentrations\n",
    "\n",
    "### Next Steps\n",
    "1. Review the generated maps in `reports/figures/`\n",
    "2. Examine specific high-value clusters for qualitative interpretation\n",
    "3. Use these spatial patterns as inputs for regression models (Notebook 04)\n",
    "4. Explore interactive visualization in the web interface\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
